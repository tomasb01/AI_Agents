"""
FLEURDIN AI - Iterativn√≠ RAG Agent
===================================

Inteligentn√≠ agent pro aromatherapii s multi-step reasoning.

Workflow:
1. Mandatory Clarification - zjist√≠ probl√©m, p≈ô√≠ƒçinu, symptomy
2. VectorSearch Loop (max 3x) - hled√° v datab√°zi esenci√°ln√≠ch olej≈Ø
3. TavilySearch Fallback (max 3x) - web search p≈ôi selh√°n√≠
4. Final Answer nebo Apology + Email notification

Author: Claude Code + Tom√°≈°
Date: 2025-12-17
"""

import json
import os
import sys
import smtplib
import numpy as np
from datetime import datetime
from typing import TypedDict, Annotated, List, Dict, Any
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

from dotenv import load_dotenv
from sklearn.metrics.pairwise import cosine_similarity

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_community.tools.tavily_search import TavilySearchResults
from sentence_transformers import SentenceTransformer

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

# ============================================
# CONFIGURATION
# ============================================

load_dotenv()

# OpenAI Configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = "gpt-4o-mini"

# Tavily Configuration
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

# Gmail SMTP Configuration
GMAIL_USER = os.getenv("GMAIL_USER")
GMAIL_APP_PASSWORD = os.getenv("GMAIL_APP_PASSWORD")
RECIPIENT_EMAIL = os.getenv("RECIPIENT_EMAIL")

# Data Configuration
DATA_PATH = "/Users/atlas/Projects/Fleurdin_AI/4-RAG_Pipeline/chunked_data_FIXED.json"

# Agent Configuration
MAX_VECTOR_ITERATIONS = 3
MAX_TAVILY_ITERATIONS = 3
RELEVANCE_THRESHOLD = 0.6  # Minimum cosine similarity score

# Embedding Model
EMBEDDING_MODEL = "paraphrase-multilingual-MiniLM-L12-v2"

# ============================================
# STATE DEFINITION
# ============================================

class AgentState(TypedDict):
    """State pro LangGraph workflow"""
    # User input
    original_question: str

    # Clarification
    has_problem: bool
    has_cause: bool
    has_symptoms: bool
    problem: str
    cause: str
    symptoms: str
    clarified_question: str

    # Vector search
    vector_search_attempts: List[Dict[str, Any]]
    vector_iteration: int
    vector_satisfied: bool

    # Tavily search
    tavily_search_attempts: List[Dict[str, Any]]
    tavily_iteration: int
    tavily_satisfied: bool

    # Results
    best_docs: List[Dict[str, Any]]
    web_context: str
    final_answer: str

    # Status
    status: str  # "success" | "failed"
    conversation_log: List[str]

# ============================================
# HELPER FUNCTIONS
# ============================================

def load_vector_data():
    """Naƒçte chunked data z JSON souboru"""
    print(f"üìÇ Loading data from: {DATA_PATH}")

    if not os.path.exists(DATA_PATH):
        print(f"‚ùå ERROR: File not found: {DATA_PATH}")
        sys.exit(1)

    with open(DATA_PATH, 'r', encoding='utf-8') as f:
        data = json.load(f)

    chunks = data['chunks']
    print(f"‚úÖ Loaded {len(chunks)} chunks")
    return chunks

def vector_search(query: str, chunks: List[dict], top_k: int = 5):
    """
    Semantic search v chunk datech pomoc√≠ cosine similarity

    Returns:
        List of tuples: [(chunk, score), ...]
    """
    # Load embedding model
    model = SentenceTransformer(EMBEDDING_MODEL)

    # Generate query embedding
    query_embedding = model.encode(query)

    # Calculate similarities
    results = []
    for chunk in chunks:
        chunk_embedding = np.array(chunk['embedding'])
        similarity = cosine_similarity(
            [query_embedding],
            [chunk_embedding]
        )[0][0]
        results.append((chunk, float(similarity)))

    # Sort by similarity (highest first)
    results.sort(key=lambda x: x[1], reverse=True)

    return results[:top_k]

def send_email_notification(state: AgentState):
    """Ode≈°le email notifikaci p≈ôi selh√°n√≠ agenta"""
    try:
        # Create message
        msg = MIMEMultipart()
        msg['From'] = GMAIL_USER
        msg['To'] = RECIPIENT_EMAIL
        msg['Subject'] = "Fleurdin AI - Nepoda≈ôilo se naj√≠t odpovƒõƒè"

        # Email body
        body = f"""
Dobr√Ω den,

Agent nebyl schopen naj√≠t uspokojivou odpovƒõƒè na dotaz u≈æivatele.

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

DOTAZ U≈ΩIVATELE:
{state['original_question']}

UP≈òESNƒöN√â INFORMACE:
- Probl√©m: {state.get('problem', 'N/A')}
- P≈ô√≠ƒçina: {state.get('cause', 'N/A')}
- Symptomy: {state.get('symptoms', 'N/A')}

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

VYZKOU≈†EN√â POKUSY:

VectorSearch ({len(state['vector_search_attempts'])}x):
"""

        for i, attempt in enumerate(state['vector_search_attempts'], 1):
            body += f"\n  {i}. Query: {attempt['query'][:100]}..."
            body += f"\n     Best score: {attempt['max_score']:.3f}"
            body += f"\n     Satisfied: {attempt['satisfied']}\n"

        body += f"\nTavilySearch ({len(state['tavily_search_attempts'])}x):\n"

        for i, attempt in enumerate(state['tavily_search_attempts'], 1):
            body += f"\n  {i}. Query: {attempt['query'][:100]}..."
            body += f"\n     Satisfied: {attempt['satisfied']}\n"

        body += "\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n"

        # Add best results found
        if state['best_docs']:
            body += "NEJLEP≈†√ç NALEZEN√â V√ùSLEDKY:\n\n"
            for i, (doc, score) in enumerate(state['best_docs'][:3], 1):
                body += f"{i}. {doc.get('name', 'Unknown')} (score: {score:.3f})\n"
                body += f"   {doc.get('text', '')[:200]}...\n\n"

        body += "\n---\nFleurdin AI Agent\n"
        body += f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"

        msg.attach(MIMEText(body, 'plain', 'utf-8'))

        # Send email
        with smtplib.SMTP('smtp.gmail.com', 587) as server:
            server.starttls()
            server.login(GMAIL_USER, GMAIL_APP_PASSWORD)
            server.send_message(msg)

        print("‚úÖ Email notification sent successfully")
        return True

    except Exception as e:
        print(f"‚ùå Failed to send email: {e}")
        return False

def save_conversation_log(state: AgentState):
    """Ulo≈æ√≠ conversation log do TXT souboru"""
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    filename = f"conversation_log_{timestamp}.txt"

    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("="*70 + "\n")
            f.write("FLEURDIN AI - CONVERSATION LOG\n")
            f.write("="*70 + "\n\n")

            f.write(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Status: {state['status']}\n\n")

            f.write("-"*70 + "\n")
            f.write("ORIGINAL QUESTION:\n")
            f.write("-"*70 + "\n")
            f.write(f"{state['original_question']}\n\n")

            f.write("-"*70 + "\n")
            f.write("CLARIFICATION:\n")
            f.write("-"*70 + "\n")
            f.write(f"Probl√©m: {state.get('problem', 'N/A')}\n")
            f.write(f"P≈ô√≠ƒçina: {state.get('cause', 'N/A')}\n")
            f.write(f"Symptomy: {state.get('symptoms', 'N/A')}\n")
            f.write(f"Clarified question: {state.get('clarified_question', 'N/A')}\n\n")

            f.write("-"*70 + "\n")
            f.write(f"VECTOR SEARCH ATTEMPTS ({len(state['vector_search_attempts'])}):\n")
            f.write("-"*70 + "\n")
            for i, attempt in enumerate(state['vector_search_attempts'], 1):
                f.write(f"\nAttempt {i}:\n")
                f.write(f"  Query: {attempt['query']}\n")
                f.write(f"  Max Score: {attempt['max_score']:.3f}\n")
                f.write(f"  Results Count: {len(attempt['results'])}\n")
                f.write(f"  Satisfied: {attempt['satisfied']}\n")

            f.write("\n" + "-"*70 + "\n")
            f.write(f"TAVILY SEARCH ATTEMPTS ({len(state['tavily_search_attempts'])}):\n")
            f.write("-"*70 + "\n")
            for i, attempt in enumerate(state['tavily_search_attempts'], 1):
                f.write(f"\nAttempt {i}:\n")
                f.write(f"  Query: {attempt['query']}\n")
                f.write(f"  Satisfied: {attempt['satisfied']}\n")

            f.write("\n" + "-"*70 + "\n")
            f.write("FINAL ANSWER:\n")
            f.write("-"*70 + "\n")
            f.write(f"{state['final_answer']}\n\n")

            f.write("="*70 + "\n")
            f.write("END OF LOG\n")
            f.write("="*70 + "\n")

        print(f"‚úÖ Conversation log saved: {filename}")
        return filename

    except Exception as e:
        print(f"‚ùå Failed to save log: {e}")
        return None

# ============================================
# LANGRAPH NODES
# ============================================

# Initialize LLM
llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0)

# Load chunks
CHUNKS = load_vector_data()

def check_clarification_node(state: AgentState) -> AgentState:
    """
    Node 1: Zkontroluje, jestli dotaz obsahuje probl√©m, p≈ô√≠ƒçinu a symptomy
    """
    print("\n" + "="*70)
    print("üîç STEP 1: Checking clarification needs...")
    print("="*70)

    messages = [
        SystemMessage(content="""Analyzuj u≈æivatelsk√Ω dotaz a zkontroluj:
1. Je specifikov√°n konkr√©tn√≠ probl√©m/pot√≠≈æ? (nap≈ô. "bol√≠ mƒõ hlava", "m√°m nespavost")
2. Je uvedena mo≈æn√° p≈ô√≠ƒçina? (nap≈ô. "kv≈Øli stresu", "po nemoci", "√∫nava")
3. Jsou uvedeny dal≈°√≠ symptomy? (nap≈ô. "a taky mƒõ bol√≠ v krku")

Odpovƒõz POUZE ve form√°tu JSON bez dal≈°√≠ho textu:
{
  "has_problem": true/false,
  "has_cause": true/false,
  "has_symptoms": true/false,
  "problem": "popis probl√©mu nebo null",
  "cause": "popis p≈ô√≠ƒçiny nebo null",
  "symptoms": "popis symptom≈Ø nebo null"
}"""),
        HumanMessage(content=f"Dotaz: {state['original_question']}")
    ]

    response = llm.invoke(messages)

    # Parse JSON response
    try:
        result = json.loads(response.content)
        state['has_problem'] = result.get('has_problem', False)
        state['has_cause'] = result.get('has_cause', False)
        state['has_symptoms'] = result.get('has_symptoms', False)
        state['problem'] = result.get('problem', '')
        state['cause'] = result.get('cause', '')
        state['symptoms'] = result.get('symptoms', '')
    except json.JSONDecodeError:
        print("‚ö†Ô∏è  Warning: Failed to parse LLM response, assuming clarification needed")
        state['has_problem'] = False
        state['has_cause'] = False
        state['has_symptoms'] = False

    print(f"\nüìä Analysis:")
    print(f"  Has problem: {state['has_problem']}")
    print(f"  Has cause: {state['has_cause']}")
    print(f"  Has symptoms: {state['has_symptoms']}")

    return state

def clarify_question_node(state: AgentState) -> AgentState:
    """
    Node 2: Interaktivnƒõ se pt√° u≈æivatele na up≈ôesnƒõn√≠
    """
    print("\n" + "="*70)
    print("üí¨ STEP 2: Asking for clarification...")
    print("="*70)

    missing_info = []
    if not state['has_problem']:
        missing_info.append("konkr√©tn√≠ probl√©m")
    if not state['has_cause']:
        missing_info.append("mo≈æn√° p≈ô√≠ƒçina")
    if not state['has_symptoms']:
        missing_info.append("dal≈°√≠ symptomy")

    print(f"\n‚ùì Rozum√≠m va≈°emu dotazu, ale pot≈ôebuji v√≠ce informac√≠:")

    # Ask for problem
    if not state['has_problem']:
        problem = input("   ‚Ä¢ Jak√Ω konkr√©tn√≠ probl√©m ≈ôe≈°√≠te? (nap≈ô. bolest hlavy, nespavost): ").strip()
        state['problem'] = problem

    # Ask for cause
    if not state['has_cause']:
        cause = input("   ‚Ä¢ V√≠te, co m≈Ø≈æe b√Ωt p≈ô√≠ƒçinou? (nap≈ô. stres, √∫nava, nemoc): ").strip()
        state['cause'] = cause

    # Ask for symptoms
    if not state['has_symptoms']:
        symptoms = input("   ‚Ä¢ M√°te i jin√© obt√≠≈æe? (pokud ne, napi≈°te 'ne'): ").strip()
        state['symptoms'] = symptoms if symptoms.lower() != 'ne' else ''

    # Create clarified question
    clarified_parts = [state['original_question']]

    if state['problem']:
        clarified_parts.append(f"Probl√©m: {state['problem']}")
    if state['cause']:
        clarified_parts.append(f"P≈ô√≠ƒçina: {state['cause']}")
    if state['symptoms']:
        clarified_parts.append(f"Dal≈°√≠ symptomy: {state['symptoms']}")

    state['clarified_question'] = ". ".join(clarified_parts)

    print(f"\n‚úÖ Clarified question: {state['clarified_question']}")

    return state

def vector_search_node(state: AgentState) -> AgentState:
    """
    Node 3: Provede vector search v datab√°zi
    """
    state['vector_iteration'] = state.get('vector_iteration', 0) + 1

    print("\n" + "="*70)
    print(f"üîé STEP 3: Vector Search (Attempt {state['vector_iteration']}/{MAX_VECTOR_ITERATIONS})")
    print("="*70)

    query = state.get('clarified_question', state['original_question'])

    print(f"\nüìù Query: {query}")
    print(f"üîç Searching in {len(CHUNKS)} chunks...")

    # Perform search
    results = vector_search(query, CHUNKS, top_k=5)

    # Extract scores
    max_score = results[0][1] if results else 0.0

    print(f"\nüìä Results:")
    print(f"  Found: {len(results)} documents")
    print(f"  Best score: {max_score:.3f}")
    print(f"  Relevance threshold: {RELEVANCE_THRESHOLD}")

    # Display top results
    print(f"\nüèÜ Top results:")
    for i, (doc, score) in enumerate(results[:3], 1):
        print(f"  {i}. {doc.get('name', 'Unknown')} (score: {score:.3f})")

    # Store attempt
    attempt = {
        'iteration': state['vector_iteration'],
        'query': query,
        'results': [(doc, score) for doc, score in results],
        'max_score': max_score,
        'satisfied': False  # Will be updated in evaluation
    }

    if 'vector_search_attempts' not in state:
        state['vector_search_attempts'] = []
    state['vector_search_attempts'].append(attempt)

    # Store best docs
    state['best_docs'] = results

    return state

def evaluate_vector_node(state: AgentState) -> AgentState:
    """
    Node 4: LLM posoud√≠, jestli jsou v√Ωsledky z VectorSearch dostaƒçuj√≠c√≠
    """
    print("\n" + "="*70)
    print("‚öñÔ∏è  STEP 4: Evaluating vector search results...")
    print("="*70)

    # Get last attempt results
    last_attempt = state['vector_search_attempts'][-1]
    results = last_attempt['results']
    max_score = last_attempt['max_score']

    # Check score threshold
    if max_score < RELEVANCE_THRESHOLD:
        print(f"\n‚ùå Score {max_score:.3f} < threshold {RELEVANCE_THRESHOLD}")
        print("   Results are not relevant enough")
        state['vector_satisfied'] = False
        last_attempt['satisfied'] = False
        return state

    # Prepare docs for LLM evaluation
    docs_text = "\n\n".join([
        f"Document {i+1} (score: {score:.3f}):\n{doc.get('text', '')[:300]}..."
        for i, (doc, score) in enumerate(results[:3])
    ])

    messages = [
        SystemMessage(content="""Jsi evalu√°tor kvality v√Ωsledk≈Ø vyhled√°v√°n√≠.
Posouƒè, jestli poskytnut√© dokumenty obsahuj√≠ dostateƒçn√© informace pro odpovƒõƒè na dotaz u≈æivatele.

Odpovƒõz POUZE ve form√°tu JSON:
{
  "satisfied": true/false,
  "reason": "kr√°tk√© zd≈Øvodnƒõn√≠"
}"""),
        HumanMessage(content=f"""Dotaz u≈æivatele: {state.get('clarified_question', state['original_question'])}

Nalezen√© dokumenty:
{docs_text}

Obsahuj√≠ tyto dokumenty dostateƒçn√© informace pro kvalitn√≠ odpovƒõƒè?""")
    ]

    response = llm.invoke(messages)

    try:
        result = json.loads(response.content)
        satisfied = result.get('satisfied', False)
        reason = result.get('reason', '')

        state['vector_satisfied'] = satisfied
        last_attempt['satisfied'] = satisfied

        print(f"\nüìä Evaluation:")
        print(f"  Satisfied: {satisfied}")
        print(f"  Reason: {reason}")

    except json.JSONDecodeError:
        print("‚ö†Ô∏è  Warning: Failed to parse evaluation, assuming not satisfied")
        state['vector_satisfied'] = False
        last_attempt['satisfied'] = False

    return state

def tavily_search_node(state: AgentState) -> AgentState:
    """
    Node 5: Provede web search pomoc√≠ Tavily
    """
    state['tavily_iteration'] = state.get('tavily_iteration', 0) + 1

    print("\n" + "="*70)
    print(f"üåê STEP 5: Tavily Web Search (Attempt {state['tavily_iteration']}/{MAX_TAVILY_ITERATIONS})")
    print("="*70)

    query = state.get('clarified_question', state['original_question'])

    print(f"\nüìù Query: {query}")
    print(f"üîç Searching web...")

    try:
        # Initialize Tavily search
        tavily = TavilySearchResults(max_results=3)
        results = tavily.invoke(query)

        # Format results
        web_context = "\n\n".join([
            f"Source {i+1}:\n{result.get('content', '')}"
            for i, result in enumerate(results)
        ])

        state['web_context'] = web_context

        print(f"\nüìä Results:")
        print(f"  Found: {len(results)} web sources")

        # Store attempt
        attempt = {
            'iteration': state['tavily_iteration'],
            'query': query,
            'results': results,
            'satisfied': False  # Will be updated in evaluation
        }

        if 'tavily_search_attempts' not in state:
            state['tavily_search_attempts'] = []
        state['tavily_search_attempts'].append(attempt)

    except Exception as e:
        print(f"‚ùå Tavily search failed: {e}")
        state['web_context'] = ""
        state['tavily_satisfied'] = False

    return state

def evaluate_tavily_node(state: AgentState) -> AgentState:
    """
    Node 6: LLM posoud√≠, jestli jsou v√Ωsledky z Tavily dostaƒçuj√≠c√≠
    """
    print("\n" + "="*70)
    print("‚öñÔ∏è  STEP 6: Evaluating Tavily search results...")
    print("="*70)

    last_attempt = state['tavily_search_attempts'][-1]

    if not state.get('web_context'):
        print("\n‚ùå No web context available")
        state['tavily_satisfied'] = False
        last_attempt['satisfied'] = False
        return state

    messages = [
        SystemMessage(content="""Jsi evalu√°tor kvality webov√Ωch v√Ωsledk≈Ø.
Posouƒè, jestli poskytnut√© webov√© zdroje obsahuj√≠ dostateƒçn√© informace pro odpovƒõƒè na dotaz.

Odpovƒõz POUZE ve form√°tu JSON:
{
  "satisfied": true/false,
  "reason": "kr√°tk√© zd≈Øvodnƒõn√≠"
}"""),
        HumanMessage(content=f"""Dotaz u≈æivatele: {state.get('clarified_question', state['original_question'])}

Webov√© zdroje:
{state['web_context'][:1000]}...

Obsahuj√≠ tyto zdroje dostateƒçn√© informace?""")
    ]

    response = llm.invoke(messages)

    try:
        result = json.loads(response.content)
        satisfied = result.get('satisfied', False)
        reason = result.get('reason', '')

        state['tavily_satisfied'] = satisfied
        last_attempt['satisfied'] = satisfied

        print(f"\nüìä Evaluation:")
        print(f"  Satisfied: {satisfied}")
        print(f"  Reason: {reason}")

    except json.JSONDecodeError:
        print("‚ö†Ô∏è  Warning: Failed to parse evaluation, assuming not satisfied")
        state['tavily_satisfied'] = False
        last_attempt['satisfied'] = False

    return state

def generate_final_answer_node(state: AgentState) -> AgentState:
    """
    Node 7: Vygeneruje fin√°ln√≠ odpovƒõƒè pro u≈æivatele
    """
    print("\n" + "="*70)
    print("‚ú® STEP 7: Generating final answer...")
    print("="*70)

    # Prepare context from vector search
    vector_context = ""
    if state.get('best_docs'):
        vector_context = "\n\n".join([
            f"{doc.get('name', 'Unknown')}:\n{doc.get('text', '')}"
            for doc, score in state['best_docs'][:3]
        ])

    # Prepare full context
    full_context = f"""DATAB√ÅZE ESENCI√ÅLN√çCH OLEJ≈Æ:
{vector_context}

WEBOV√â ZDROJE:
{state.get('web_context', '≈Ω√°dn√© webov√© zdroje')}"""

    messages = [
        SystemMessage(content="""Jsi zku≈°en√Ω aromaterapeut a expert na p≈ô√≠rodn√≠ medic√≠nu.

D≈ÆLE≈ΩIT√â POKYNY:
1. Odpov√≠dej POUZE na z√°kladƒõ poskytnut√©ho kontextu
2. Odpov√≠daj STEJN√ùM JAZYKEM jako ot√°zka u≈æivatele (ƒçe≈°tina/sloven≈°tina)
3. Pou≈æ√≠vej p≈ôirozen√Ω, vst≈ô√≠cn√Ω t√≥n - jako expert kter√Ω rad√≠
4. Doporuƒç KOMBINACI esenci√°ln√≠ch olej≈Ø A bylinn√Ωch p≈ô√≠pravk≈Ø (pokud jsou v kontextu)
5. Pro ka≈æd√© doporuƒçen√≠ uveƒè:
   - Konkr√©tn√≠ n√°zvy (esenci√°ln√≠ oleje + bylinky)
   - Jak je pou≈æ√≠vat (inhalace, mas√°≈æ, difuz√©r, ƒçaj, tinktura)
   - P≈ô√≠padn√° upozornƒõn√≠
6. Nepi≈° to jako seznam z datab√°ze, ale jako radu od zku≈°en√©ho terapeuta"""),
        HumanMessage(content=f"""Klient se pt√°: {state.get('clarified_question', state['original_question'])}

Kontext:
{full_context}

Poskytni kompletn√≠, p≈ô√°telskou a odbornou odpovƒõƒè:""")
    ]

    response = llm.invoke(messages)
    state['final_answer'] = response.content
    state['status'] = 'success'

    print("\n‚úÖ Final answer generated")

    return state

def generate_apology_node(state: AgentState) -> AgentState:
    """
    Node 8: Omluva u≈æivateli + odesl√°n√≠ emailu + logging
    """
    print("\n" + "="*70)
    print("üòî STEP 8: Generating apology and sending notification...")
    print("="*70)

    # Generate apology message
    apology = f"""Omlouv√°me se, ale nepoda≈ôilo se n√°m naj√≠t uspokojivou odpovƒõƒè na v√°≈° dotaz.

V√°≈° dotaz: {state['original_question']}

Probl√©m: {state.get('problem', 'N/A')}
P≈ô√≠ƒçina: {state.get('cause', 'N/A')}
Symptomy: {state.get('symptoms', 'N/A')}

Vyzkou≈°eli jsme:
‚Ä¢ {len(state.get('vector_search_attempts', []))}x hled√°n√≠ v na≈°√≠ datab√°zi esenci√°ln√≠ch olej≈Ø
‚Ä¢ {len(state.get('tavily_search_attempts', []))}x webov√© vyhled√°v√°n√≠

Bohu≈æel jsme nena≈°li dostateƒçnƒõ relevantn√≠ informace pro kvalitn√≠ odpovƒõƒè.

Pros√≠m zanechte n√°m zde sv≈Øj kontakt na email ƒçi mobil, ozveme se V√°m zpƒõt. 

V√°≈° t√Ωm Fleurdin AI"""

    state['final_answer'] = apology
    state['status'] = 'failed'

    print("\nüìß Sending email notification...")
    email_sent = send_email_notification(state)

    if email_sent:
        print("‚úÖ Email sent successfully")
    else:
        print("‚ùå Failed to send email")

    return state

# ============================================
# CONDITIONAL EDGES
# ============================================

def route_after_clarification(state: AgentState) -> str:
    """Rozhodne, jestli pot≈ôebujeme clarification"""
    needs_clarification = not (
        state.get('has_problem', False) and
        state.get('has_cause', False) and
        state.get('has_symptoms', False)
    )

    if needs_clarification:
        print("\n‚û°Ô∏è  Route: Need clarification")
        return "clarify"
    else:
        print("\n‚û°Ô∏è  Route: Skip clarification, go to vector search")
        # Set clarified_question same as original
        state['clarified_question'] = state['original_question']
        return "vector_search"

def route_after_vector_evaluation(state: AgentState) -> str:
    """Rozhodne co dƒõlat po Vector Search evaluation"""

    if state.get('vector_satisfied', False):
        print("\n‚û°Ô∏è  Route: Vector search satisfied ‚Üí Final answer")
        return "generate_final_answer"

    if state.get('vector_iteration', 0) < MAX_VECTOR_ITERATIONS:
        print(f"\n‚û°Ô∏è  Route: Try vector search again ({state['vector_iteration']}/{MAX_VECTOR_ITERATIONS})")
        return "ask_refinement"

    print("\n‚û°Ô∏è  Route: Max vector iterations reached ‚Üí Try Tavily")
    return "tavily_search"

def route_after_tavily_evaluation(state: AgentState) -> str:
    """Rozhodne co dƒõlat po Tavily Search evaluation"""

    if state.get('tavily_satisfied', False):
        print("\n‚û°Ô∏è  Route: Tavily search satisfied ‚Üí Final answer")
        return "generate_final_answer"

    if state.get('tavily_iteration', 0) < MAX_TAVILY_ITERATIONS:
        print(f"\n‚û°Ô∏è  Route: Try Tavily again ({state['tavily_iteration']}/{MAX_TAVILY_ITERATIONS})")
        return "tavily_search"

    print("\n‚û°Ô∏è  Route: Max Tavily iterations reached ‚Üí Apology + Email")
    return "generate_apology"

def ask_user_refinement_node(state: AgentState) -> AgentState:
    """Zept√° se u≈æivatele na up≈ôesnƒõn√≠ dotazu pro dal≈°√≠ VectorSearch"""
    print("\n" + "="*70)
    print("üí¨ Asking user for query refinement...")
    print("="*70)

    print("\n‚ùì Nena≈°li jsme dostateƒçnƒõ relevantn√≠ v√Ωsledky.")
    print("   M≈Ø≈æete pros√≠m up≈ôesnit v√°≈° dotaz nebo p≈ôidat v√≠ce informac√≠?")

    refinement = input("\n   Up≈ôesnƒõn√≠: ").strip()

    # Update clarified question
    if refinement:
        state['clarified_question'] = f"{state['clarified_question']}. {refinement}"

    print(f"\n‚úÖ Updated query: {state['clarified_question']}")

    return state

# ============================================
# BUILD LANGGRAPH WORKFLOW
# ============================================

def build_graph():
    """Sestav√≠ LangGraph workflow"""

    workflow = StateGraph(AgentState)

    # Add nodes
    workflow.add_node("check_clarification", check_clarification_node)
    workflow.add_node("clarify_question", clarify_question_node)
    workflow.add_node("vector_search", vector_search_node)
    workflow.add_node("evaluate_vector", evaluate_vector_node)
    workflow.add_node("ask_refinement", ask_user_refinement_node)
    workflow.add_node("tavily_search", tavily_search_node)
    workflow.add_node("evaluate_tavily", evaluate_tavily_node)
    workflow.add_node("generate_final_answer", generate_final_answer_node)
    workflow.add_node("generate_apology", generate_apology_node)

    # Add edges
    workflow.add_edge(START, "check_clarification")

    # Conditional: clarification needed?
    workflow.add_conditional_edges(
        "check_clarification",
        route_after_clarification,
        {
            "clarify": "clarify_question",
            "vector_search": "vector_search"
        }
    )

    workflow.add_edge("clarify_question", "vector_search")
    workflow.add_edge("vector_search", "evaluate_vector")

    # Conditional: after vector evaluation
    workflow.add_conditional_edges(
        "evaluate_vector",
        route_after_vector_evaluation,
        {
            "generate_final_answer": "generate_final_answer",
            "ask_refinement": "ask_refinement",
            "tavily_search": "tavily_search"
        }
    )

    workflow.add_edge("ask_refinement", "vector_search")
    workflow.add_edge("tavily_search", "evaluate_tavily")

    # Conditional: after tavily evaluation
    workflow.add_conditional_edges(
        "evaluate_tavily",
        route_after_tavily_evaluation,
        {
            "generate_final_answer": "generate_final_answer",
            "tavily_search": "tavily_search",
            "generate_apology": "generate_apology"
        }
    )

    workflow.add_edge("generate_final_answer", END)
    workflow.add_edge("generate_apology", END)

    return workflow.compile()

# ============================================
# MAIN EXECUTION
# ============================================

def main():
    """Hlavn√≠ funkce"""
    print("\n" + "="*70)
    print("üåø FLEURDIN AI - Iterativn√≠ RAG Agent")
    print("="*70)

    # Validate configuration
    if not OPENAI_API_KEY:
        print("‚ùå ERROR: OPENAI_API_KEY not found in .env")
        sys.exit(1)

    if not TAVILY_API_KEY:
        print("‚ö†Ô∏è  WARNING: TAVILY_API_KEY not found in .env")
        print("   Tavily search will not work")

    if not GMAIL_USER or not GMAIL_APP_PASSWORD:
        print("‚ö†Ô∏è  WARNING: Gmail credentials not found in .env")
        print("   Email notifications will not work")

    # Build graph
    print("\nüìä Building workflow graph...")
    graph = build_graph()
    print("‚úÖ Graph built successfully")

    # Get user question
    print("\n" + "-"*70)
    question = input("\nüí¨ Zadejte v√°≈° dotaz (nebo 'exit' pro ukonƒçen√≠): ").strip()

    if question.lower() in ['exit', 'quit', 'konec']:
        print("\nüëã Nashledanou!")
        return

    # Initialize state
    initial_state = {
        'original_question': question,
        'vector_search_attempts': [],
        'tavily_search_attempts': [],
        'vector_iteration': 0,
        'tavily_iteration': 0,
        'conversation_log': []
    }

    # Run workflow
    print("\nüöÄ Starting agent workflow...\n")

    try:
        final_state = graph.invoke(initial_state)

        # Display final answer
        print("\n" + "="*70)
        print("üìù FINAL ANSWER:")
        print("="*70)
        print(f"\n{final_state['final_answer']}\n")

        # Save conversation log
        print("\n" + "-"*70)
        print("üíæ Saving conversation log...")
        log_file = save_conversation_log(final_state)

        if log_file:
            print(f"‚úÖ Log saved to: {log_file}")

        print("\n" + "="*70)
        print(f"‚úÖ Session completed with status: {final_state['status']}")
        print("="*70 + "\n")

    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Session interrupted by user")
        sys.exit(0)

    except Exception as e:
        print(f"\n\n‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
